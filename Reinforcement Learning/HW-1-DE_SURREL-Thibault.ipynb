{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAE8kMxe6E6k"
   },
   "source": [
    "# MVA - Homework 1 - Reinforcement Learning (2022/2023)\n",
    "\n",
    "**Name:** DE SURREL Thibault"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vY4MH0nU637o"
   },
   "source": [
    "## Instructions\n",
    "\n",
    "* The deadline is **November 10 at 11:59 pm (Paris time).**\n",
    "\n",
    "* By doing this homework you agree to the late day policy, collaboration and misconduct rules reported on [Piazza](https://piazza.com/class/l4y5ubadwj64mb/post/6).\n",
    "\n",
    "* **Mysterious or unsupported answers will not receive full credit**. A correct answer, unsupported by calculations, explanation, or algebraic work will receive no credit; an incorrect answer supported by substantially correct calculations and explanations might still receive partial credit.\n",
    "\n",
    "* Answers should be provided in **English**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YB__2uUC5U1r"
   },
   "source": [
    "# Colab setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XNj1_VZ2FGJ"
   },
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  # install rlberry library\n",
    "  !pip install git+https://github.com/rlberry-py/rlberry.git@mva2021#egg=rlberry[default] > /dev/null 2>&1\n",
    "\n",
    "  # install ffmpeg-python for saving videos\n",
    "  !pip install ffmpeg-python > /dev/null 2>&1\n",
    "\n",
    "  # packages required to show video\n",
    "  !pip install pyvirtualdisplay > /dev/null 2>&1\n",
    "  !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
    "\n",
    "  print(\"Libraries installed, please restart the runtime!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8F7RiPXjutB"
   },
   "outputs": [],
   "source": [
    "# Create directory for saving videos\n",
    "!mkdir videos > /dev/null 2>&1\n",
    "\n",
    "# Initialize display and import function to show videos\n",
    "import rlberry.colab_utils.display_setup\n",
    "from rlberry.colab_utils.display_setup import show_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KISV44N_nCNm"
   },
   "outputs": [],
   "source": [
    "# Useful libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4hKBRTCh6Gk"
   },
   "source": [
    "# Preparation\n",
    "\n",
    "In the coding exercises, you will use a *grid-world* MDP, which is represented in Python using the interface provided by the [Gym](https://gym.openai.com/) library. The cells below show how to interact with this MDP and how to visualize it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "514mHDeQooKa"
   },
   "outputs": [],
   "source": [
    "from rlberry.envs import GridWorld\n",
    "\n",
    "def get_env():\n",
    "  \"\"\"Creates an instance of a grid-world MDP.\"\"\"\n",
    "  env = GridWorld(\n",
    "      nrows=5,\n",
    "      ncols=7,\n",
    "      reward_at = {(0, 6):1.0},\n",
    "      walls=((0, 4), (1, 4), (2, 4), (3, 4)),\n",
    "      success_probability=0.9,\n",
    "      terminal_states=((0, 6),)\n",
    "  )\n",
    "  return env\n",
    "\n",
    "def render_policy(env, policy=None, horizon=50):\n",
    "  \"\"\"Visualize a policy in an environment\n",
    "\n",
    "  Args:\n",
    "    env: GridWorld\n",
    "        environment where to run the policy\n",
    "    policy: np.array\n",
    "        matrix mapping states to action (Ns).\n",
    "        If None, runs random policy.\n",
    "    horizon: int\n",
    "        maximum number of timesteps in the environment.\n",
    "  \"\"\"\n",
    "  env.enable_rendering()\n",
    "  state = env.reset()                       # get initial state\n",
    "  for timestep in range(horizon):\n",
    "      if policy is None:\n",
    "        action = env.action_space.sample()  # take random actions\n",
    "      else:\n",
    "        action = policy[state]\n",
    "      next_state, reward, is_terminal, info = env.step(action)\n",
    "      state = next_state\n",
    "      if is_terminal:\n",
    "        break\n",
    "  # save video and clear buffer\n",
    "  env.save_video('./videos/gw.mp4', framerate=5)\n",
    "  env.clear_render_buffer()\n",
    "  env.disable_rendering()\n",
    "  # show video\n",
    "  show_video('./videos/gw.mp4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQAHUBw_ifMI"
   },
   "outputs": [],
   "source": [
    "# Create an environment and visualize it\n",
    "env = get_env()\n",
    "render_policy(env)  # visualize random policy\n",
    "\n",
    "# The reward function and transition probabilities can be accessed through\n",
    "# the R and P attributes:\n",
    "print(f\"Shape of the reward array = (S, A) = {env.R.shape}\")\n",
    "print(f\"Shape of the transition array = (S, A, S) = {env.P.shape}\")\n",
    "print(f\"Reward at (s, a) = (1, 0): {env.R[1, 0]}\")\n",
    "print(f\"Prob[s\\'=2 | s=1, a=0]: {env.P[1, 0, 2]}\")\n",
    "print(f\"Number of states and actions: {env.Ns}, {env.Na}\")\n",
    "\n",
    "# The states in the griworld correspond to (row, col) coordinates.\n",
    "# The environment provides a mapping between (row, col) and the index of\n",
    "# each state:\n",
    "print(f\"Index of state (1, 0): {env.coord2index[(1, 0)]}\")\n",
    "print(f\"Coordinates of state 5: {env.index2coord[5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibGD_3I89CNu"
   },
   "source": [
    "# Part 1 - Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NR7h5won9NQY"
   },
   "source": [
    "## Question 1.1\n",
    "\n",
    "Consider a general MDP with a discount factor of $\\gamma < 1$. Assume that the horizon is infinite (so there is no termination). A policy $\\pi$ in this MDP\n",
    "induces a value function $V^\\pi$. Suppose an affine transformation is applied to the reward, what is\n",
    "the new value function? Is the optimal policy preserved?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "313W4K3B_LtN"
   },
   "source": [
    "### **Answer**\n",
    "\n",
    "Let $f \\colon x \\mapsto ax + b$ be the affine transformation with $a,b \\in \\mathbb{R}$. \n",
    "Then, the new value fonction $V^\\pi_f$ is : $$V^\\pi_f(s) = \\mathbb{E}\\left[\\sum\\limits_{t = 0}^\\infty \\gamma^t f(r(s_t,a_t)) \\mid s_0 = s, a_t \\sim d_t(h_t), \\pi \\right] = a V^\\pi(s) + \\frac{b}{1-\\gamma}$$\n",
    "If $a > 0$, the optimal policy is preserved because the $argmax$ is the same. \n",
    "If $a \\leq 0$, the optimal policy is not preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uCVgkDo9vTM"
   },
   "source": [
    "## Question 1.2\n",
    "\n",
    "Consider an infinite-horizon $\\gamma$-discounted MDP. We denote by $Q^*$ the $Q$-function of the optimal policy $\\pi^*$. Prove that, for any function $Q(s, a)$ (which is **not** necessarily the value function of a policy), the following inequality holds for any state $s$:\n",
    "\n",
    "$$\n",
    "V^{\\pi_Q}(s) \\geq V^*(s) - \\frac{2}{1-\\gamma}||Q^*-Q||_\\infty,\n",
    "$$\n",
    "\n",
    "where $||Q^*-Q||_\\infty = \\max_{s, a} |Q^*(s, a) - Q(s, a)|$ and $\\pi_Q(s) \\in \\arg\\max_a Q(s, a)$. Can you use this result to show that any policy $\\pi$ such that $\\pi(s) \\in \\arg\\max_a Q^*(s, a)$ is optimal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MqGWPPD_OAI"
   },
   "source": [
    "### **Answer**\n",
    "\n",
    "Let $s$ be a state. We have :\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "Q^*(s,\\pi^*(s)) - Q^*(s,\\pi_Q(s)) &= Q^*(s,\\pi^*(s)) - Q(s,\\pi^*(s)) + Q(s,\\pi^*(s)) - Q^*(s,\\pi_Q(s)) \\\\\n",
    "&\\leq \\|Q^* - Q \\|_{\\infty} + Q(s,\\pi^*(s)) - Q^*(s,\\pi_Q(s))\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "By definition of $\\pi_Q(s)$, we have that \n",
    "$Q(s,\\pi^*(s)) \\leq Q(s,\\pi_Q(s))$ so, \n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "Q^*(s,\\pi^*(s)) - Q^*(s,\\pi_Q(s)) &\\leq \\|Q^* - Q \\|_{\\infty} + Q(s,\\pi_Q(s)) - Q^*(s,\\pi_Q(s)) \\\\\n",
    "&\\leq  2 \\|Q^* - Q \\|_{\\infty}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Let $Q^{\\pi_Q}$ be the $Q$-function of the policy $\\pi_Q$. We then remark that we have, thank to the Bellman equation : \n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "Q^*(s,\\pi^*(s)) &= r(s,\\pi^*(s)) + \\gamma \\sum_{s'}p(s,\\pi^*(s),s')V^*(s') = V^*(s) \\\\\n",
    "Q^{\\pi_Q}(s,\\pi_Q(s)) &= r(s,\\pi_Q(s)) + \\gamma \\sum_{s'}p(s,\\pi_Q(s),s')V^{\\pi_Q}(s') = V^{\\pi_Q}(s)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "So, when considering the first inequality shown :  \n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "2 \\|Q^* - Q \\|_{\\infty} &\\geq Q^*(s,\\pi^*(s)) - Q^*(s,\\pi_Q(s)) \\\\\n",
    "&\\geq Q^*(s,\\pi^*(s)) - Q^{\\pi_Q}(s,\\pi_Q(s)) + Q^{\\pi_Q}(s,\\pi_Q(s)) - Q^*(s,\\pi_Q(s)) \\\\\n",
    "&\\geq V^*(s) - V^{\\pi_Q}(s) + r(s,\\pi_Q(s)) + \\gamma \\sum_{s'}p(s,\\pi_Q(s),s')V^{\\pi_Q}(s') - r(s,\\pi_Q(s)) + \\gamma \\sum_{s'}p(s,\\pi_Q(s),s')V^*(s') \\\\\n",
    "&\\geq  V^*(s) - V^{\\pi_Q}(s) + \\gamma\\sum_{s'}p(s,\\pi_Q(s),s')[V^{\\pi_Q}(s') - V^*(s')]\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Let $s_0$ be a state such that $V^*(s_0) - V^{\\pi_Q}(s_0) = \\max_s V^*(s) - V^{\\pi_Q}(s)$. Then, for all states $s$, we have $V^*(s_0) - V^{\\pi_Q}(s_0) \\leq V^{\\pi_Q}(s) - V^*(s)$. Especially, ther former inequality holds for all states $s$, we have :  \n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "2 \\|Q^* - Q \\|_{\\infty} &\\geq  V^*(s_0) - V^{\\pi_Q}(s_0) + \\gamma \\sum_{s'}p(s_0,\\pi_Q(s_0),s')[V^{\\pi_Q}(s') - V^*(s')]\\\\\n",
    "&\\geq V^*(s_0) - V^{\\pi_Q}(s_0) + \\gamma \\left[\\sum_{s'}p(s_0,\\pi_Q(s_0),s')\\right](V^{\\pi_Q}(s_0) - V^*(s_0)) \\\\\n",
    "&\\geq (1 - \\gamma )(V^*(s_0) - V^{\\pi_Q}(s_0))\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "We then have the result, for all state $s$ : \n",
    "$$ V^*(s) - V^{\\pi_Q}(s) \\leq V^*(s_0) - V^{\\pi_Q}(s_0) \\leq \\frac{2}{1-\\gamma}\\|Q^* - Q \\|_{\\infty} $$\n",
    "Finally : \n",
    "$$V^{\\pi_Q}(s) \\geq V^*(s) - \\frac{2}{1-\\gamma}\\|Q^* - Q \\|_{\\infty}$$\n",
    "\n",
    "\n",
    " Let $\\pi$ be a policy such that $\\pi(s) \\in \\arg\\max_a Q^*(s, a)$. Then, with previous notations, we have $\\pi = \\pi_{Q^*}$. So, by the former inequality : \n",
    "$$V^{\\pi}(s) = V^{\\pi_{Q^*}}(s) \\geq V^*(s) - \\frac{2}{1-\\gamma}\\|Q^* - Q^* \\|_{\\infty} \\geq V^*(s)$$ \n",
    "The policy is therefor optimal.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIrtb7sihYcM"
   },
   "source": [
    "## Question 1.3\n",
    "\n",
    "In this question, you will implement and compare the policy and value iteration algorithms for a finite MDP. \n",
    "\n",
    "Complete the functions `policy_evaluation`, `policy_iteration` and `value_iteration` below.\n",
    "\n",
    "\n",
    "Compare value iteration and policy iteration. Highlight pros and cons of each method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLmQtk-wt0HS"
   },
   "source": [
    "### **Answer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9yI0YYtMmpDQ"
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(P, R, policy, gamma=0.9, tol=1e-2):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        P: np.array\n",
    "            transition matrix (NsxNaxNs)\n",
    "        R: np.array\n",
    "            reward matrix (NsxNa)\n",
    "        policy: np.array\n",
    "            matrix mapping states to action (Ns)\n",
    "        gamma: float\n",
    "            discount factor\n",
    "        tol: float\n",
    "            precision of the solution\n",
    "    Return:\n",
    "        value_function: np.array\n",
    "            The value function of the given policy\n",
    "    \"\"\"\n",
    "    n = 1000\n",
    "    Ns, Na = R.shape\n",
    "    value_function = np.zeros(Ns)\n",
    "    # ====================================================\n",
    "\n",
    "    delta = 1 + tol\n",
    "    while delta > tol:\n",
    "      delta = 0\n",
    "      for s in range(Ns):\n",
    "        a = policy[s]\n",
    "\n",
    "        new_value = R[s,a] + gamma * np.sum([P[s,a,s_p]*value_function[s_p] for s_p in range(Ns)])\n",
    "        delta = max(delta,np.abs(new_value - value_function[s]))\n",
    "        value_function[s] = new_value\n",
    "\n",
    "    # ====================================================\n",
    "    return value_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ncqbPx99ncVY"
   },
   "outputs": [],
   "source": [
    "def policy_iteration(P, R, gamma=0.9, tol=1e-3):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        P: np.array\n",
    "            transition matrix (NsxNaxNs)\n",
    "        R: np.array\n",
    "            reward matrix (NsxNa)\n",
    "        gamma: float\n",
    "            discount factor\n",
    "        tol: float\n",
    "            precision of the solution\n",
    "    Return:\n",
    "        policy: np.array\n",
    "            the final policy\n",
    "        V: np.array\n",
    "            the value function associated to the final policy\n",
    "        number_it : int\n",
    "            The number of iterations it took to compute the value fonction\n",
    "    \"\"\"\n",
    "    Ns, Na = R.shape\n",
    "    old_V = np.zeros(Ns)\n",
    "    policy = np.ones(Ns, dtype=int)\n",
    "    number_it = 0\n",
    "    # ====================================================\n",
    "\t  # YOUR IMPLEMENTATION HERE \n",
    "    #\n",
    "    V = policy_evaluation(P, R, policy, gamma=gamma, tol=tol)\n",
    "    \n",
    "    delta = tol + 1\n",
    "\n",
    "    while delta > tol:\n",
    "      delta = 0\n",
    "      number_it += 1\n",
    "      old_V = np.copy(V)\n",
    "      policy = np.array([np.argmax(R[s] + gamma*np.dot(P[s],V)) for s in range(Ns)])\n",
    "\n",
    "      V = policy_evaluation(P, R, policy, gamma=gamma, tol=tol)\n",
    "      \n",
    "      delta = np.linalg.norm(V - old_V)\n",
    "    # ====================================================\n",
    "    return policy, V, number_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3jB7dfA5nRCZ"
   },
   "outputs": [],
   "source": [
    "def value_iteration(P, R, gamma=0.9, tol=1e-3):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        P: np.array\n",
    "            transition matrix (NsxNaxNs)\n",
    "        R: np.array\n",
    "            reward matrix (NsxNa)\n",
    "        gamma: float\n",
    "            discount factor\n",
    "        tol: float\n",
    "            precision of the solution\n",
    "    Return:\n",
    "        Q: final Q-function (at iteration n)\n",
    "        greedy_policy: greedy policy wrt Qn\n",
    "        Qfs: all Q-functions generated by the algorithm (for visualization)\n",
    "        number_it : int\n",
    "            The number of iterations it took to compute the value fonction\n",
    "    \"\"\"\n",
    "    Ns, Na = R.shape\n",
    "    Q = np.zeros((Ns, Na))\n",
    "    Qfs = [Q]\n",
    "    number_it = 0\n",
    "    # ====================================================\n",
    "    delta = 1 + tol\n",
    "    while delta > tol:\n",
    "      number_it += 1\n",
    "      Q_new = R + gamma * np.dot(P,np.max(Q,axis=1)) \n",
    "      delta = np.amax(np.abs(Q-Q_new))\n",
    "      Q = np.copy(Q_new)\n",
    "      Qfs.append(np.copy(Q_new))\n",
    "    \n",
    "    greedy_policy = np.argmax(Q, axis=1)\n",
    "    # ====================================================\n",
    "    return Q, greedy_policy, Qfs, number_it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Fi0IzZJp74Z"
   },
   "source": [
    "### Testing your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P7JKrc1oqFI2"
   },
   "outputs": [],
   "source": [
    "from time import time \n",
    "# Parameters\n",
    "tol = 1e-5\n",
    "gamma = 0.99\n",
    "\n",
    "# Environment\n",
    "env = get_env()\n",
    "\n",
    "# run value iteration to obtain Q-values\n",
    "t = time()\n",
    "VI_Q, VI_greedypol, all_qfunctions, number_it_VI = value_iteration(env.P, env.R, gamma=gamma, tol=tol)\n",
    "print(\"Time to compute the value iteration : \" + str(time() - t ))\n",
    "print(\"Number of iterations for the value iteration : \" + str(number_it_VI))\n",
    "\n",
    "# render the policy\n",
    "print(\"[VI]Greedy policy: \")\n",
    "render_policy(env, VI_greedypol)\n",
    "\n",
    "# compute the value function of the greedy policy using matrix inversion\n",
    "# ====================================================\n",
    "greedy_V = policy_evaluation(env.P, env.R, VI_greedypol, gamma= gamma, tol= tol )\n",
    "\n",
    "# ====================================================\n",
    "\n",
    "# show the error between the computed V-functions and the final V-function\n",
    "# (that should be the optimal one, if correctly implemented)\n",
    "# as a function of time\n",
    "final_V = all_qfunctions[-1].max(axis=1)\n",
    "norms = [ np.linalg.norm(q.max(axis=1) - final_V) for q in all_qfunctions]\n",
    "plt.plot(norms)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Error')\n",
    "plt.title(\"Value iteration: convergence\")\n",
    "\n",
    "#### POLICY ITERATION ####\n",
    "t = time()\n",
    "\n",
    "PI_policy, PI_V, number_it_PI = policy_iteration(env.P, env.R, gamma=gamma, tol=tol)\n",
    "print(\"Time to compute the poilcy iteration : \" + str(time() - t ))\n",
    "print(\"Number of iterations for the policy iteration : \" + str(number_it_PI))\n",
    "\n",
    "print(\"\\n[PI]final policy: \")\n",
    "render_policy(env, PI_policy)\n",
    "\n",
    "## Uncomment below to check that everything is correct\n",
    "assert np.allclose(PI_policy, VI_greedypol),\\\n",
    "     \"You should check the code, the greedy policy computed by VI is not equal to the solution of PI\"\n",
    "assert np.allclose(PI_V, greedy_V),\\\n",
    "     \"Since the policies are equal, even the value function should be\"\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kG_vt8FxAVM6"
   },
   "source": [
    "**Answer**\n",
    "\n",
    "**In theory** we have the following pros et cons for each method : \n",
    "\n",
    "|                    **Policy iteration**                   |                    **Value iteration**                   |\n",
    "|:---------------------------------------------------------:|:--------------------------------------------------------:|\n",
    "| Algorithm is more complex since we need policy evaluation | Simpler algorithm                                        |\n",
    "| Cheaper to compute                                        | More expensive to compute since we have a max to compute |\n",
    "| Faster to converge                                        | Slower to converge                                       |\n",
    "| The convergence is guaranteed                             | The convergence is guaranteed                            |\n",
    "\n",
    "**In practice** we can see that the value iteration is a lot quicker than the policy iteration ($\\sim$ 0.04 secondes for the value iteration versus 4.8 secondes for the policy iteration). But we also see that the policy iteration needs a lot less iterations (only 4) to converge versus the value iteration (1147 iterations). In this example, the computation time of each iteration is thus a lot quicker for value iteration than for policy iteraiton. In fact, at each step, the policy iteration needs to compute a policy evaluation, and in this take, computing an $argmax$ like in the value iteration is quicker. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2V1QdoH-xFX0"
   },
   "source": [
    "# Part 2 - Tabular RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qf51VhoPxbV4"
   },
   "source": [
    "## Question 2.1\n",
    "\n",
    "The code below collects two datasets of transitions (containing states, actions, rewards and next states) for a discrete MDP.\n",
    "\n",
    "For each of the datasets:\n",
    "\n",
    "1. Estimate the transitions and rewards, $\\hat{P}$ and $\\hat{R}$.\n",
    "2. Compute the optimal value function and the optimal policy with respect to the estimated MDP (defined by $\\hat{P}$ and $\\hat{R}$), which we denote by $\\hat{\\pi}$ and $\\hat{V}$.\n",
    "3. Numerically compare the performance of $\\hat{\\pi}$ and $\\pi^\\star$ (the true optimal policy), and the error between $\\hat{V}$ and $V^*$ (the true optimal value function).\n",
    "\n",
    "Which of the two data collection methods do you think is better? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWSyewG2EZpJ"
   },
   "source": [
    "### **Answer**\n",
    "\n",
    "[answer last question + implementation below]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8lNPhB28EcGd"
   },
   "outputs": [],
   "source": [
    "def get_random_policy_dataset(env, n_samples):\n",
    "  \"\"\"Get a dataset following a random policy to collect data.\"\"\"\n",
    "  states = []\n",
    "  actions = []\n",
    "  rewards = []\n",
    "  next_states = []\n",
    "  \n",
    "  state = env.reset()\n",
    "  for _ in range(n_samples):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, is_terminal, info = env.step(action)\n",
    "    states.append(state)\n",
    "    actions.append(action)\n",
    "    rewards.append(reward)\n",
    "    next_states.append(next_state)\n",
    "    # update state\n",
    "    state = next_state\n",
    "    if is_terminal:\n",
    "      state = env.reset()\n",
    "\n",
    "  dataset = (states, actions, rewards, next_states)\n",
    "  return dataset\n",
    "\n",
    "def get_uniform_dataset(env, n_samples):\n",
    "  \"\"\"Get a dataset by uniformly sampling states and actions.\"\"\"\n",
    "  states = []\n",
    "  actions = []\n",
    "  rewards = []\n",
    "  next_states = []\n",
    "  for _ in range(n_samples):\n",
    "    state = env.observation_space.sample()\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, is_terminal, info = env.sample(state, action)\n",
    "    states.append(state)\n",
    "    actions.append(action)\n",
    "    rewards.append(reward)\n",
    "    next_states.append(next_state)\n",
    "\n",
    "  dataset = (states, actions, rewards, next_states)\n",
    "  return dataset\n",
    "\n",
    "\n",
    "# Collect two different datasets\n",
    "num_samples = 500\n",
    "env = get_env()\n",
    "dataset_1 = get_random_policy_dataset(env, num_samples)\n",
    "dataset_2 = get_uniform_dataset(env, num_samples)\n",
    "\n",
    "\n",
    "# Item 3: Estimate the MDP with the two datasets; compare the optimal value\n",
    "# functions in the true and in the estimated MDPs\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wgx2C3Htg3_H"
   },
   "outputs": [],
   "source": [
    "Ns, Na = env.R.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WDq3o3D9giXG"
   },
   "outputs": [],
   "source": [
    "def estimate_p(dataset, Ns, Na, num_samples):\n",
    "  (states, actions, rewards, next_states) = dataset\n",
    "  N = np.zeros((Ns,Na,Ns))  \n",
    "  p_hat = np.zeros((Ns,Na,Ns))\n",
    "  for i in range(num_samples):\n",
    "    N[states[i],actions[i],next_states[i]] += 1\n",
    "  \n",
    "  for s in range(Ns):\n",
    "    for a in range(Na):\n",
    "      div = np.sum([N[s,a,i] for i in range(Ns)])\n",
    "      if div != 0:\n",
    "          p_hat[s,a] =  N[s,a]/div\n",
    "  return p_hat\n",
    "\n",
    "def estimate_r(dataset, Ns, Na, num_samples):\n",
    "  (states, actions, rewards, next_states) = dataset\n",
    "  r_hat = np.zeros((Ns,Na))\n",
    "  N = np.zeros((Ns,Na,Ns))  \n",
    "  for i in range(num_samples):\n",
    "    N[states[i],actions[i],next_states[i]] += 1\n",
    "    r_hat[states[i],actions[i]] += rewards[i]\n",
    "  \n",
    "  for s in range(Ns):\n",
    "    for a in range(Na):\n",
    "      div = np.sum([N[s,a,i] for i in range(Ns)])\n",
    "      if div != 0:\n",
    "          r_hat[s,a] /= div\n",
    "  return r_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fcGBvc78jWUP"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "#For dataset_1\n",
    "p_hat_1 = estimate_p(dataset_1, Ns, Na, num_samples)\n",
    "r_hat_1 = estimate_r(dataset_1, Ns, Na, num_samples)\n",
    "\n",
    "pi_hat_1, V_hat_1, _ = policy_iteration(p_hat_1, r_hat_1)\n",
    "pi_star_1, V_star_1, _ = policy_iteration(env.P,env.R)\n",
    "\n",
    "mae_1_V = mean_absolute_error(V_hat_1,V_star_1)  \n",
    "mse_1_V = mean_squared_error(V_hat_1,V_star_1)  \n",
    "\n",
    "#For dataset_2\n",
    "p_hat_2 = estimate_p(dataset_2, Ns, Na, num_samples)\n",
    "r_hat_2 = estimate_r(dataset_2, Ns, Na, num_samples)\n",
    "\n",
    "pi_hat_2, V_hat_2, _ = policy_iteration(p_hat_2, r_hat_2)\n",
    "pi_star_2, V_star_2, _ = policy_iteration(env.P,env.R)\n",
    "\n",
    "mae_2_V = mean_absolute_error(V_hat_2,V_star_2)  \n",
    "mse_2_V = mean_squared_error(V_hat_2,V_star_2)  \n",
    "\n",
    "print(\"Results for dataset_1 : \")\n",
    "print(\"Mean square error for V : \", mse_1_V)\n",
    "print(\"Mean absolute error for V : \", mae_1_V)\n",
    "\n",
    "\n",
    "print(\"Results for dataset_2 : \")\n",
    "print(\"Mean square error for V : \", mse_2_V)\n",
    "print(\"Mean absolute error for V : \", mae_2_V)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVRjEPSXv-y-"
   },
   "source": [
    "**Answer**\n",
    "\n",
    "Intuitively, the second dataset is better than the first one. In fact, the uniform sampling chooses a random state and action at each step whereas the random policy datasat chooses a random policy and follows it. So the uniform sampling allows us to explore more datapoints. \n",
    "\n",
    "The results of the experiments confirm this idea. In fact, the mean absolute error and the mean squared error are a lot better for dataset 2 than dataset 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKINsa_yGLGL"
   },
   "source": [
    "## Question 2.2\n",
    "\n",
    "Suppose that $\\hat{P}$ and $\\hat{R}$ are estimated from a dataset of exactly $N$ i.i.d. samples from **each** state-action pair. This means that, for each $(s,a)$, we have $N$ samples $\\{(s_1',r_1, \\dots, s_N', r_N\\}$, where $s_i' \\sim P(\\cdot | s,a)$ and $r_i \\sim R(s,a)$ for $i=1,\\dots,N$, and\n",
    "$$ \\hat{P}(s'|s,a) = \\frac{1}{N}\\sum_{i=1}^N \\mathbb{1}(s_i' = s'), $$\n",
    "$$ \\hat{R}(s,a) = \\frac{1}{N}\\sum_{i=1}^N r_i.$$\n",
    "Suppose that $R$ is a distribution with support in $[0,1]$. Let $\\hat{V}$ be the optimal value function computed in the empirical MDP (i.e., the one with transitions $\\hat{P}$ and rewards $\\hat{R}$). For any $\\delta\\in(0,1)$, derive an upper bound to the error\n",
    "\n",
    "$$ \\| \\hat{V} - V^* \\|_\\infty $$\n",
    "\n",
    "which holds with probability at least $1-\\delta$.\n",
    "\n",
    "**Note** Your bound should only depend on deterministic quantities like $N$, $\\gamma$, $\\delta$, $S$, $A$. It should *not* dependent on the actual random samples.\n",
    "\n",
    "**Hint** The following two inequalities may be helpful.\n",
    "\n",
    "1. **A (simplified) lemma**. For any state $\\bar{s}$,\n",
    "\n",
    "$$ |\\hat{V}(\\bar{s}) - V^*(\\bar{s})| \\leq \\frac{1}{1-\\gamma}\\max_{s,a} \\left| R(s,a) - \\hat{R}(s,a) + \\gamma \\sum_{s'}(P(s'|s,a) - \\hat{P}(s'|s,a)) V^*(s') \\right|$$\n",
    "\n",
    "2. **Hoeffding's inequality**. Let $X_1, \\dots X_N$ be $N$ i.i.d. random variables bounded in the interval $[0,b]$ for some $b>0$. Let $\\bar{X} = \\frac{1}{N}\\sum_{i=1}^N X_i$ be the empirical mean. Then, for any $\\epsilon > 0$,\n",
    "\n",
    "$$ \\mathbb{P}(|\\bar{X} - \\mathbb{E}[\\bar{X}]| > \\epsilon) \\leq 2e^{-\\frac{2N\n",
    "\\epsilon^2}{b^2}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKmdulLaMoiN"
   },
   "source": [
    "### **Answer**\n",
    "\n",
    "Let $(s,a)$ be a state-action pair. We then define $X_i^{s,a}$ by : \n",
    "$$X_i^{s,a} = r_i + \\gamma \\sum\\limits_{s'} \\mathbb{1}_{s_i' = s'}V^*(s')$$\n",
    "The $(X_i^{s,a})_{i}$ are $N$ i.i.d. random variables because $r_i$ and $s_i'$ are. There are bounded in the the interval $[0, \\frac{1}{1-\\gamma}]$. In fact, we have $|r_i| \\leq 1$ so :\n",
    "$$|X_i^{s,a}| \\leq |r_i| +  \\gamma \\sum\\limits_{s'} \\mathbb{1}_{s_i' = s'}|V^*(s)| \\leq 1 +  \\gamma \\sum\\limits_{s'}|V^*(s')|$$\n",
    "Moreover \n",
    "$$|V^*(s)| \\leq \\mathbb{E}\\left[\\sum\\limits_{t=0}^{\\infty} \\gamma^t |R(s_t,a_t)| \\mid s_0 = s\\right] \\leq \\sum\\limits_{t=0}^{\\infty} \\gamma^t \\leq \\frac{1}{1-\\gamma}$$\n",
    "So, we have $|X_i^{a,b}| \\leq 1 + \\frac{\\gamma}{1-\\gamma} =  \\frac{1}{1-\\gamma}$\n",
    "\n",
    "Denoting $\\bar{X}^{s,a} = \\frac{1}{N}\\sum\\limits_{i=1}^N X_i^{s,a}$, we have \n",
    "$$\\bar{X}^{s,a} = \\frac{1}{N}\\sum\\limits_{i=1}^N r_i + \\gamma \\sum\\limits_{s'}\\frac{1}{N}\\sum\\limits_{i=1}^N \\mathbb{1}_{s_i' = s'}V^*(s') = \\hat{R}(s,a) + \\gamma\\sum\\limits_{s'} \\hat{P}(s'\\mid s,a) V^*(s') $$\n",
    "and \n",
    "$$\\mathbb{E}[\\bar{X}^{s,a}] = R(s,a) + \\gamma\\sum\\limits_{s'} P(s'\\mid s,a) V^*(s') $$\n",
    "So, we have \n",
    "$$|\\bar{X}^{s,a} - \\mathbb{E}[\\bar{X}^{s,a}]| = | \\hat{R}(s,a) - R(s,a) + \\gamma \\sum\\limits_{s'} (\\hat{P}(s'\\mid s,a) - P(s'\\mid s,a) ) V^*(s')|$$\n",
    "We can now use Hoeffding's inequality : for $\\varepsilon > 0$ to be determined,\n",
    "$$\\mathbb{P}\\left( |\\bar{X}^{s,a} - \\mathbb{E}[\\bar{X}^{s,a}]| \\leq \\varepsilon\\right) \\geq 1 - 2e^{-2(1-\\gamma)^2N\\varepsilon^2}$$\n",
    "Then, using le lemme we have, for all state $\\bar{s}$, \n",
    "$$\\|\\hat{V} - V^* \\|_{\\infty} \\leq |\\hat{V}(\\bar{s}) - V^*(\\bar{s})| \\leq \\frac{1}{1-\\gamma}\\max_{s,a} |\\bar{X}^{s,a} - \\mathbb{E}[\\bar{X}^{s,a}]| $$\n",
    "So, we have :\n",
    "$$\\mathbb{P}\\left(\\|\\hat{V} - V^* \\|_{\\infty} \\leq  \\varepsilon\\right) \\geq \\mathbb{P}\\left(\\frac{1}{1-\\gamma}\\max_{s,a} |\\bar{X}^{s,a} - \\mathbb{E}[\\bar{X}^{s,a}]| \\leq  \\varepsilon\\right)$$\n",
    "Using the independance of our samples, we have :\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\mathbb{P}\\left(\\max_{s,a} |\\bar{X}^{s,a} - \\mathbb{E}[\\bar{X}^{s,a}]| \\leq  (1-\\gamma)\\varepsilon\\right) &= \\mathbb{P}\\left(\\bigcap_{s,a} \\left[|\\bar{X}^{s,a} - \\mathbb{E}[\\bar{X}^{s,a}]| \\leq  (1-\\gamma)\\varepsilon\\right]\\right)\\\\ &= \\prod_{s,a} \\mathbb{P}\\left(|\\bar{X}^{s,a} - \\mathbb{E}[\\bar{X}^{s,a}]| \\leq  (1-\\gamma)\\varepsilon\\right) \\\\\n",
    "&\\geq  \\left( 1 - 2e^{-2(1-\\gamma)^4N\\varepsilon^2}\\right)^{|S||A|}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "The last inequality comming from the Hoeffding's inequality of above. We can then take :\n",
    "$$\\varepsilon = \\sqrt{- \\frac{\\log \\left(\\frac{1 - (1 - \\delta)^{\\frac{1}{|S||A|}}}{2}\\right)}{2(1-\\gamma)^4N}}$$\n",
    "we finally have :\n",
    "$$\\mathbb{P}\\left(\\|\\hat{V} - V^* \\|_{\\infty} \\leq  \\varepsilon\\right) \\geq 1 - \\delta$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tpqwCBG2MwxO"
   },
   "source": [
    "## Question 2.3\n",
    "\n",
    "Suppose once again that we are given a dataset of $N$ samples in the form of tuples $(s_i,a_i,s_i',r_i)$. We know that each tuple contains a valid transition from the true MDP, i.e., $s_i' \\sim P(\\cdot | s_i, a_i)$ and $r_i \\sim R(s_i,a_i)$, while the state-action pairs $(s_i,a_i)$ from which the transition started can be arbitrary.\n",
    "\n",
    "Suppose we want to apply Q-learning to this MDP. Can you think of a way to leverage this offline data to improve the sample-efficiency of the algorithm? What if we were using SARSA instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbYTKetHOYU_"
   },
   "source": [
    "### **Answer**\n",
    "\n",
    "- For the Q-learning, we can use the Tabular Dyna-Q algorithm to have a better sample-efficiency. In this algorithm, the main idea is to plan with Model-based learning andto learning with Model-free learning. In practice, we update our model at each step, so that we are closer to the real one and thus improve the sample-efficiency. \n",
    "- For SARSA, we can use an importance sampling methode to improve the sample-efficiency. This means that we use a seighted distribution that helps us favour \"important\" samples over less important ones. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "542QxKsSOs21"
   },
   "source": [
    "# Part 3 - RL with Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiGZBiJ4PiIE"
   },
   "source": [
    "## Question 3.1\n",
    "\n",
    "Given a datset $(s_i, a_i, r_i, s_i')$ of (states, actions, rewards, next states), the Fitted Q-Iteration (FQI) algorithm proceeds as follows:\n",
    "\n",
    "\n",
    "* We start from a $Q$ function $Q_0 \\in \\mathcal{F}$, where $\\mathcal{F}$ is a function space;\n",
    "* At every iteration $k$, we compute $Q_{k+1}$ as:\n",
    "\n",
    "$$\n",
    "Q_{k+1}\\in\\arg\\min_{f\\in\\mathcal{F}} \\frac{1}{2}\\sum_{i=1}^N\n",
    "\\left(\n",
    "  f(s_i, a_i) - y_i^k\n",
    "\\right)^2 + \\lambda \\Omega(f)\n",
    "$$\n",
    "where $y_i^k = r_i + \\gamma \\max_{a'}Q_k(s_i', a')$, $\\Omega(f)$ is a regularization term and $\\lambda > 0$ is the regularization coefficient.\n",
    "\n",
    "\n",
    "Consider FQI with *linear* function approximation. That is, for a given feature map $\\phi : S \\rightarrow \\mathbb{R}^d$, we consider a parametric family of $Q$ functions $Q_\\theta(s,a) = \\phi(s)^T\\theta_a$ for $\\theta_a\\in\\mathbb{R}^d$. Suppose we are applying FQI on a given dataset of $N$ tuples of the form $(s_i, a_i, r_i, s_i')$ and we are at the $k$-th iteration. Let $\\theta_k \\in\\mathbb{R}^{d \\times A}$ be our current parameter. Derive the *closed-form* update to find $\\theta_{k+1}$, using $\\frac{1}{2}\\sum_a ||\\theta_a||_2^2$ as regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jx7aE41DkEM"
   },
   "source": [
    "### **Answer**\n",
    "\n",
    "According that we consider the FQI with linear function approximation we can rewrite the defintion of $Q_{k+1}$ as :\n",
    "\n",
    "$$Q_{k+1} \\in \\arg\\min_{\\theta \\in \\mathbb{R}^{d\\times A}} \\frac{1}{2} \\sum_{i=1}^N \\left( \\phi(s_i)^T \\theta_{a_i} - y_i^k \\right)^2 + \\frac{1}{2}\\lambda \\sum_{i=1}^N \\| \\theta_a\\|_2^2$$\n",
    "Let us define \n",
    "$$\\forall \\theta \\in \\mathbb{R}^{d\\times A}, ~\\Psi(\\theta) =  \\frac{1}{2} \\sum_{i=1}^N \\left( \\phi(s_i)^T \\theta_{a_i} - y_i^k \\right)^2 + \\frac{1}{2}\\lambda \\sum_{i=1}^N \\| \\theta_a\\|_2^2$$\n",
    "To find the minimum of $\\Psi$, let $a$ be a action. We will compute the gradient of $\\Psi$ according to $a$ : $\\nabla_{\\theta_a}\\Psi$ : \n",
    "$$\\nabla_{\\theta_a}\\Psi(\\theta)  = \\sum\\limits_{\\substack{i=0 \\\\ s.t.~a_i = a}}^N \\left( (\\phi(s_i)^T \\theta_{a_i} - y_i^k )\\phi(s_i) + \\lambda \\theta_{a_i} \\right) $$\n",
    "So, \n",
    "$$\\nabla_{\\theta_a}\\Psi(\\theta) = 0 \\iff \\theta_a = \\left(\\sum\\limits_{\\substack{i=0 \\\\ s.t. a_i = a}}^N \\phi(s_i)\\phi(s_i)^T + \\lambda I_d \\right)^{-1} \\sum\\limits_{\\substack{i=0 \\\\ s.t.~a_i = a}}^N y_i^k \\phi(s_i)$$\n",
    "\n",
    "So, we have $\\theta_{k+1}$ such that, for all $a$,\n",
    "$$\\theta_{k+1,a} = \\left(\\sum\\limits_{\\substack{i=0 \\\\ s.t. a_i = a}}^N \\phi(s_i)\\phi(s_i)^T + \\lambda I_d \\right)^{-1} \\sum\\limits_{\\substack{i=0 \\\\ s.t.~a_i = a}}^N y_i^k \\phi(s_i)$$\n",
    "And, $$Q_{k+1}(s,a) = \\phi(s)^T\\theta_{k+1,a}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewHzjm7MVGBg"
   },
   "source": [
    "## Question 3.2\n",
    "\n",
    "The code below creates a larger gridworld (with more states than the one used in the previous questions), and defines a feature map. Implement linear FQI to this environment (in the function `linear_fqi()` below), and compare the approximated $Q$ function to the optimal $Q$ function computed with value iteration.\n",
    "\n",
    "Can you improve the feature map in order to reduce the approximation error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tu4g-HSnEcBs"
   },
   "source": [
    "### **Answer**\n",
    "\n",
    "[explanation about how you tried to reduce the approximation error + FQI implementation below]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZovF3VXOVfCs"
   },
   "outputs": [],
   "source": [
    "def get_large_gridworld():\n",
    "  \"\"\"Creates an instance of a grid-world MDP with more states.\"\"\"\n",
    "  walls = [(ii, 10) for ii in range(15) if (ii != 7 and ii != 8)]\n",
    "  env = GridWorld(\n",
    "      nrows=15,\n",
    "      ncols=15,\n",
    "      reward_at = {(14, 14):1.0},\n",
    "      walls=tuple(walls),\n",
    "      success_probability=0.9,\n",
    "      terminal_states=((14, 14),)\n",
    "  )\n",
    "  return env\n",
    "\n",
    "\n",
    "class GridWorldFeatureMap:\n",
    "  \"\"\"Create features for state-action pairs\n",
    "  \n",
    "  Args:\n",
    "    dim: int\n",
    "      Feature dimension\n",
    "    sigma: float\n",
    "      RBF kernel bandwidth\n",
    "  \"\"\"\n",
    "  def __init__(self, env, dim=15, sigma=0.25):\n",
    "    self.index2coord = env.index2coord\n",
    "    self.n_states = env.Ns\n",
    "    self.n_actions = env.Na\n",
    "    self.dim = dim\n",
    "    self.sigma = sigma\n",
    "\n",
    "    n_rows = env.nrows\n",
    "    n_cols = env.ncols\n",
    "\n",
    "    # build similarity matrix\n",
    "    sim_matrix = np.zeros((self.n_states, self.n_states))\n",
    "    for ii in range(self.n_states):\n",
    "        row_ii, col_ii = self.index2coord[ii]\n",
    "        x_ii = row_ii / n_rows\n",
    "        y_ii = col_ii / n_cols\n",
    "        for jj in range(self.n_states):\n",
    "            row_jj, col_jj = self.index2coord[jj]\n",
    "            x_jj = row_jj / n_rows\n",
    "            y_jj = col_jj / n_cols\n",
    "            dist = np.sqrt((x_jj - x_ii) ** 2.0 + (y_jj - y_ii) ** 2.0)\n",
    "            sim_matrix[ii, jj] = np.exp(-(dist / sigma) ** 2.0)\n",
    "\n",
    "    # factorize similarity matrix to obtain features\n",
    "    uu, ss, vh = np.linalg.svd(sim_matrix, hermitian=True)\n",
    "    self.feats = vh[:dim, :]\n",
    "\n",
    "  def map(self, observation):\n",
    "    feat = self.feats[:, observation].copy()\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "InCfu7F9-TbS"
   },
   "outputs": [],
   "source": [
    "env = get_large_gridworld()\n",
    "feat_map = GridWorldFeatureMap(env)\n",
    "\n",
    "# Visualize large gridworld\n",
    "render_policy(env)\n",
    "\n",
    "# The features have dimension (feature_dim).\n",
    "feature_example = feat_map.map(1) # feature representation of s=1\n",
    "print(feature_example)\n",
    "\n",
    "# Initial vector theta representing the Q function\n",
    "theta = np.zeros((feat_map.dim, env.action_space.n))\n",
    "print(theta.shape)\n",
    "print(feature_example @ theta) # approximation of Q(s=1, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p21KMmruugO1"
   },
   "outputs": [],
   "source": [
    "def linear_fqi(env, feat_map, num_iterations, lambd=0.1, gamma=0.95):\n",
    "  \"\"\"\n",
    "  # Linear FQI implementation\n",
    "  # TO BE COMPLETED\n",
    "  \"\"\"\n",
    "\n",
    "  N = 5000\n",
    "  Ns, Na = env.R.shape\n",
    "\n",
    "  # get a dataset\n",
    "  dataset = get_uniform_dataset(env, n_samples = N)\n",
    "  # dataset = get_random_policy_dataset(env, n_samples=...)\n",
    "  (states, actions, rewards, next_states) = dataset\n",
    "\n",
    "  theta = np.zeros((feat_map.dim, env.Na))\n",
    "  Rmax = np.max(rewards)\n",
    "  for it in range(num_iterations):\n",
    "    #Compute y^k_i :\n",
    "    y_k = np.zeros(N)\n",
    "    for i in range(N):\n",
    "      Q_k = [np.sum(feat_map.map(next_states[i])*theta[:,a]) for a in range(Na)]\n",
    "      f = np.max(Q_k)\n",
    "      #clipping \n",
    "      if f > 0 :\n",
    "        f = min(f , Rmax / (1-gamma) )\n",
    "      else : \n",
    "        f_s_a = max(f, - Rmax/(1-gamma))\n",
    "\n",
    "      y_k[i] = rewards[i] + gamma * f\n",
    "    \n",
    "    #Create theta_{k+1}\n",
    "    theta_new = np.zeros((feat_map.dim, env.Na))\n",
    "\n",
    "    #For each action a\n",
    "    for a in range(Na):\n",
    "      #First we want to get the indices i such that a_i = a:\n",
    "      #indices = np.where(np.array(actions) == a)[0]\n",
    "\n",
    "      indices = []\n",
    "      for i in range(N):\n",
    "        if actions[i] == a:\n",
    "          indices.append(i)\n",
    "\n",
    "      if len(indices) != 0:\n",
    "        to_be_inv = np.sum([np.matmul(np.array([feat_map.map(states[i])]).T, np.array([feat_map.map(states[i])])) for i in indices],axis = 0) + lambd*np.eye(feat_map.dim)\n",
    "\n",
    "        #to_be_inv = np.sum([np.matmul(np.transpose(np.asmatrix(feat_map.map(states[i]))),np.asmatrix(feat_map.map(states[i]))) for i in indices],axis=0) + lambd*np.eye(feat_map.dim)\n",
    "        #print(to_be_inv.shape)\n",
    "        sum = np.sum([y_k[i]*feat_map.map(states[i]) for i in indices],axis=0)\n",
    "        #print(sum.shape)\n",
    "        theta_new[:,a] = np.matmul(np.linalg.inv(to_be_inv),sum)\n",
    "    \n",
    "    theta = np.copy(theta_new)\n",
    "  \n",
    "  return theta\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Environment and feature map\n",
    "# ----------------------------\n",
    "env = get_large_gridworld()\n",
    "# you can change the parameters of the feature map, and even try other maps!\n",
    "feat_map = GridWorldFeatureMap(env, dim=50, sigma=0.25)\n",
    "\n",
    "# -------\n",
    "# Run FQI\n",
    "# -------\n",
    "theta = linear_fqi(env, feat_map, num_iterations=100)\n",
    "\n",
    "# Compute and run greedy policy\n",
    "Q_fqi = np.zeros((env.Ns, env.Na))\n",
    "for ss in range(env.Ns):\n",
    "  state_feat = feat_map.map(ss)\n",
    "  Q_fqi[ss, :] = state_feat @ theta\n",
    "\n",
    "V_fqi = Q_fqi.max(axis=1)\n",
    "policy_fqi = Q_fqi.argmax(axis=1)\n",
    "render_policy(env, policy_fqi, horizon=100)\n",
    "\n",
    "# Visualize the approximate value function in the gridworld.\n",
    "img = env.get_layout_img(V_fqi)    \n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CcLDtoCgIbuW"
   },
   "outputs": [],
   "source": [
    "#Comparaison betwenn Q_fqi and Q from the value iteration\n",
    "VI_Q, VI_greedypol, all_qfunctions, number_it_VI = value_iteration(env.P, env.R)\n",
    "\n",
    "mae_Q = mean_absolute_error(VI_Q,Q_fqi)  \n",
    "mse_Q = mean_squared_error(VI_Q,Q_fqi)  \n",
    "print(\"Mean square error for Q : \", mse_Q)\n",
    "print(\"Mean absolute error for Q : \", mae_Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRMbwa8zLABE"
   },
   "source": [
    "We see that the mean squared error and the mean absolute error between the  approximated $Q$ function to the optimal $Q$ function computed with value iteration are quite low. But when we compare the value maps from the value iteration and the one from function approximation, we can see that they quite lookalike but the function appromixation one has higher values ferther away from the objectif. So qualitatively, the red diamond will find its way to the objectif, but the value function will be slightly higher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uH4cfC0zLzTp"
   },
   "outputs": [],
   "source": [
    "V_VI = VI_Q.max(axis=1)\n",
    "plt.subplot(1,2,1)\n",
    "img_VI = env.get_layout_img(V_VI)    \n",
    "plt.imshow(img_VI)\n",
    "plt.title(\"Value iteration\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img)\n",
    "plt.title(\"Function approximation\")\n",
    "plt.suptitle(\"Value maps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7irvDEZ1Rq4B"
   },
   "outputs": [],
   "source": [
    "#Influence of sigma and dim : \n",
    "sigma = np.linspace(0.1,1,10)\n",
    "dim = np.linspace(10,100,10,dtype=int)\n",
    "\n",
    "mse = np.zeros((10,10))\n",
    "mae = np.zeros((10,10))\n",
    "\n",
    "VI_Q, VI_greedypol, all_qfunctions, number_it_VI = value_iteration(env.P, env.R)\n",
    "\n",
    "\n",
    "for s in range(10):\n",
    "  for d in range(10):\n",
    "    feat_map = GridWorldFeatureMap(env, dim=dim[d], sigma=sigma[s])\n",
    "\n",
    "    theta = linear_fqi(env, feat_map, num_iterations=100)\n",
    "\n",
    "    # Compute and run greedy policy\n",
    "    Q_fqi = np.zeros((env.Ns, env.Na))\n",
    "    for ss in range(env.Ns):\n",
    "      state_feat = feat_map.map(ss)\n",
    "      Q_fqi[ss, :] = state_feat @ theta\n",
    "\n",
    "\n",
    "    mae[d,s] = (mean_absolute_error(VI_Q,Q_fqi))\n",
    "    mse[d,s] = (mean_squared_error(VI_Q,Q_fqi))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWoQqFktEAZj"
   },
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.pcolormesh(mae)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"sigma\")\n",
    "plt.ylabel(\"Dim\")\n",
    "plt.title(\"Mean absolute error\")\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.pcolormesh(mse)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"sigma\")\n",
    "plt.ylabel(\"Dim\")\n",
    "plt.title(\"Mean squared error\")\n",
    "\n",
    "plt.suptitle(\"Error between Q for value iteration and Q from function approximation\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKRWTirVFaq-"
   },
   "source": [
    "I have computed the MSE and MAE between Q for value iteration and Q from function approximation for different values of the dimension $d$ of $\\phi$ and $\\sigma$. This way, I hope finding the best combination of those two values in order to minimize this error. The minimum is obtained for $d = 80$ and $\\sigma = 0.1$. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
